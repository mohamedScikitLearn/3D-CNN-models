{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5acd1acb",
   "metadata": {},
   "source": [
    "Sources: \n",
    " https://github.com/DLTK/DLTK/tree/master/examples/tutorials\n",
    " \n",
    " \n",
    " https://github.com/fitushar/3DCNNs_TF2Modelhub\n",
    " \n",
    " \n",
    " https://keras.io/examples/vision/3D_image_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import optimizers,layers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "\n",
    "initializer = tf.keras.initializers.HeUniform()\n",
    "tf.random.set_seed(42)\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8977882c",
   "metadata": {},
   "source": [
    "## Downloading and processing the CT (3D.niftii files) dataset\n",
    "Here I will be using the same tutorial presented in https://keras.io/examples/vision/3D_image_classification/\n",
    "to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download url of normal CT scans.\n",
    "url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-0.zip\"\n",
    "filename = os.path.join(os.getcwd(), \"CT-0.zip\")\n",
    "keras.utils.get_file(filename, url)\n",
    "\n",
    "# Download url of abnormal CT scans.\n",
    "url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-23.zip\"\n",
    "filename = os.path.join(os.getcwd(), \"CT-23.zip\")\n",
    "keras.utils.get_file(filename, url)\n",
    "\n",
    "# Make a directory to store the data.\n",
    "os.makedirs(\"MosMedData\")\n",
    "\n",
    "# Unzip data in the newly created directory.\n",
    "with zipfile.ZipFile(\"CT-0.zip\", \"r\") as z_fp:\n",
    "    z_fp.extractall(\"./MosMedData/\")\n",
    "\n",
    "with zipfile.ZipFile(\"CT-23.zip\", \"r\") as z_fp:\n",
    "    z_fp.extractall(\"./MosMedData/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "\n",
    "def normalise_zero_one(image):\n",
    "    \"\"\"Image normalisation. Normalises image to fit [0, 1] range.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    minimum = np.min(image)\n",
    "    maximum = np.max(image)\n",
    "    if maximum > minimum:\n",
    "        ret = (image - minimum) / (maximum - minimum)\n",
    "    else:\n",
    "        ret = image * 0.\n",
    "    return ret\n",
    "\n",
    "def whitening(image):\n",
    "    \"\"\"Whitening. Normalises image to zero mean and unit variance.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    if std > 0:\n",
    "        ret = (image - mean) / std\n",
    "    else:\n",
    "        ret = image * 0.\n",
    "    return ret\n",
    "\n",
    "def remove_slices(scan):\n",
    "    scan = scan[:,:,35:135]\n",
    "    return scan\n",
    "\n",
    "def crop3D(scan): ## Bonus Function,\n",
    "    #this could be useful for many MRI datasets where the beginning of the MRI series are usually useless\n",
    "    start = (90,90)     \n",
    "    end = (290,290)\n",
    "    slices = tuple(map(slice, start, end))\n",
    "    return scan[slices] \n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 64\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = whitening(volume)\n",
    "    \n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "\n",
    "    return volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4be805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder \"CT-0\" consist of CT scans having normal lung tissue,\n",
    "# no CT-signs of viral pneumonia.\n",
    "normal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"MosMedData/CT-0\", x)\n",
    "    for x in os.listdir(\"MosMedData/CT-0\")\n",
    "]\n",
    "# Folder \"CT-23\" consist of CT scans having several ground-glass opacifications,\n",
    "# involvement of lung parenchyma.\n",
    "abnormal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"MosMedData/CT-23\", x)\n",
    "    for x in os.listdir(\"MosMedData/CT-23\")\n",
    "]\n",
    "\n",
    "print(\"CT scans with normal lung tissue: \" + str(len(normal_scan_paths)))\n",
    "print(\"CT scans with abnormal lung tissue: \" + str(len(abnormal_scan_paths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd69f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process the scans.\n",
    "# Each scan is resized across height, width, and depth and rescaled.\n",
    "abnormal_scans = np.array([process_scan(path) for path in abnormal_scan_paths])\n",
    "normal_scans = np.array([process_scan(path) for path in normal_scan_paths])\n",
    "\n",
    "# For the CT scans having presence of viral pneumonia\n",
    "# assign 1, for the normal ones assign 0.\n",
    "abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])\n",
    "normal_labels = np.array([0 for _ in range(len(normal_scans))])\n",
    "\n",
    "\n",
    "data= np.concatenate((abnormal_scans, normal_scans), axis=0)\n",
    "label = np.concatenate((abnormal_labels, normal_labels), axis=0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, label, test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume\n",
    "\n",
    "\n",
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "batch_size = 2\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01cf55",
   "metadata": {},
   "source": [
    "# VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def VGG3D(inputs):\n",
    "    inputs = inputs\n",
    "    x = inputs\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "\n",
    "    if TRAIN_CLASSIFY_USE_BN:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "\n",
    "    if TRAIN_CLASSIFY_USE_BN:\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(128, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(128, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(128, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "\n",
    "    if TRAIN_CLASSIFY_USE_BN:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(256, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(256, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(256, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "\n",
    "    if TRAIN_CLASSIFY_USE_BN:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(512, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(512, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv3D(512, (3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling3D()(x)\n",
    "\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(units=1,activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "model=VGG3D(layers.Input((128, 128, 64,1)))\n",
    "\n",
    "\n",
    "## This section will be the same for all the classifiers. \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.Precision(),tf.keras.metrics.AUC()])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309f714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68bc21d7",
   "metadata": {},
   "source": [
    "# Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81572251",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCEPTION_BLOCKS = 6\n",
    "INCEPTION_REDUCTION_STEPS = 2\n",
    "INCEPTION_KEEP_FILTERS = 128\n",
    "INCEPTION_ENABLE_DEPTHWISE_SEPARABLE_CONV_SHRINKAGE = 0.333\n",
    "INCEPTION_ENABLE_SPATIAL_SEPARABLE_CONV = True\n",
    "INCEPTION_DROPOUT = 0.5\n",
    "\n",
    "def conv_bn_relu(x, filters, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same'):\n",
    "    x = layers.Conv3D(filters, kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def inception_base(x):\n",
    "    x = conv_bn_relu(x, filters=32)\n",
    "    x = conv_bn_relu(x, filters=32)\n",
    "    x = conv_bn_relu(x, filters=64)\n",
    "\n",
    "    b0 = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    b1 = conv_bn_relu(x, 64, strides=(2, 2, 2))\n",
    "    x =  layers.Concatenate(axis=4)([b0, b1])\n",
    "\n",
    "    print('inception_base')\n",
    "    print(b0.get_shape())\n",
    "    print(b1.get_shape())\n",
    "    print(x.get_shape())\n",
    "\n",
    "    return x\n",
    "\n",
    "def inception_block(x, filters=256):\n",
    "    shrinkaged_filters = int(filters * INCEPTION_ENABLE_DEPTHWISE_SEPARABLE_CONV_SHRINKAGE)\n",
    "    b0 = conv_bn_relu(x, filters=filters, kernel_size=(1, 1, 1))\n",
    "\n",
    "    b1 = conv_bn_relu(x, filters=shrinkaged_filters, kernel_size=(1, 1, 1))\n",
    "    b1 = conv_bn_relu(b1, filters=filters, kernel_size=(3, 3, 3))\n",
    "\n",
    "    b2 = conv_bn_relu(x, filters=shrinkaged_filters, kernel_size=(1, 1, 1))\n",
    "    b2 = conv_bn_relu(b2, filters=filters, kernel_size=(3, 3, 3))\n",
    "    b2 = conv_bn_relu(b2, filters=filters, kernel_size=(3, 3, 3))\n",
    "\n",
    "    b3 = layers.AveragePooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), padding='same')(x)\n",
    "    b3 = conv_bn_relu(b3, filters=filters, kernel_size=(1, 1, 1))\n",
    "\n",
    "    bs = [b0, b1, b2, b3]\n",
    "\n",
    "    print('inception_block')\n",
    "    print(b0.get_shape())\n",
    "    print(b1.get_shape())\n",
    "    print(b2.get_shape())\n",
    "    print(b3.get_shape())\n",
    "\n",
    "    if INCEPTION_ENABLE_SPATIAL_SEPARABLE_CONV:\n",
    "        b4 = conv_bn_relu(x, filters=shrinkaged_filters, kernel_size=(1, 1, 1))\n",
    "        b4 = conv_bn_relu(b4, filters=filters, kernel_size=(5, 1, 1))\n",
    "        b4 = conv_bn_relu(b4, filters=filters, kernel_size=(1, 5, 1))\n",
    "        b4 = conv_bn_relu(b4, filters=filters, kernel_size=(1, 1, 5))\n",
    "        bs.append(b4)\n",
    "        print(b4.get_shape())\n",
    "\n",
    "    x = layers.Concatenate(axis=4)(bs)\n",
    "    print(x.get_shape())\n",
    "\n",
    "    return x\n",
    "\n",
    "def reduction_block(x, filters=256):\n",
    "    b0 = conv_bn_relu(x, filters=filters, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding='same')\n",
    "\n",
    "    b1 = conv_bn_relu(x, filters=filters, kernel_size=(1, 1, 1))\n",
    "    b1 = conv_bn_relu(b1, filters=filters, kernel_size=(3, 3, 3))\n",
    "    b1 = conv_bn_relu(b1, filters=filters, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding='same')\n",
    "\n",
    "    b2 = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same')(x)\n",
    "    b2 = conv_bn_relu(b2, filters=filters, kernel_size=(1, 1, 1))\n",
    "\n",
    "    bs = [b0, b1, b2]\n",
    "\n",
    "    print('reduction_block')\n",
    "    print(b0.get_shape())\n",
    "    print(b1.get_shape())\n",
    "    print(b2.get_shape())\n",
    "\n",
    "    if INCEPTION_ENABLE_SPATIAL_SEPARABLE_CONV:\n",
    "        b3 = conv_bn_relu(x, filters=filters, kernel_size=(1, 1, 1))\n",
    "        b3 = conv_bn_relu(b3, filters=filters, kernel_size=(5, 1, 1))\n",
    "        b3 = conv_bn_relu(b3, filters=filters, kernel_size=(1, 5, 1))\n",
    "        b3 = conv_bn_relu(b3, filters=filters, kernel_size=(1, 1, 5))\n",
    "        b3 = conv_bn_relu(b3, filters=filters, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding='same')\n",
    "        bs.append(b3)\n",
    "        print(b3.get_shape())\n",
    "\n",
    "    x = Concatenate(axis=4)(bs)\n",
    "    print(x.get_shape())\n",
    "\n",
    "    return x\n",
    "\n",
    "def Inception3D(inputs,num_classes):\n",
    "    inputs = inputs\n",
    "    # Make inception base\n",
    "    x = inception_base(inputs)\n",
    "\n",
    "    for i in range(INCEPTION_BLOCKS):\n",
    "        x = inception_block(x, filters=INCEPTION_KEEP_FILTERS)\n",
    "\n",
    "        if (i + 1) % INCEPTION_REDUCTION_STEPS == 0 and i != INCEPTION_BLOCKS - 1:\n",
    "            x = reduction_block(x, filters=INCEPTION_KEEP_FILTERS // 2)\n",
    "\n",
    "    print('top')\n",
    "    x = layers.GlobalMaxPooling3D()(x)\n",
    "    x = layers.Dropout(INCEPTION_DROPOUT)(x)\n",
    "    x = layers.Dense(units=1,activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model=Inception3D(layers.Input((128, 128, 64,1)))\n",
    "\n",
    "\n",
    "## This section will be the same for all the classifiers. \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.Precision(),tf.keras.metrics.AUC()])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156bd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ed1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM_RES_UNIT=3\n",
    "TRAIN_NUM_FILTERS=(16, 32, 64, 128)\n",
    "TRAIN_STRIDES=((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2))\n",
    "TRAIN_CLASSIFY_ACTICATION=tf.nn.relu6\n",
    "TRAIN_KERNAL_INITIALIZER=tf.keras.initializers.VarianceScaling(distribution='uniform')\n",
    "\n",
    "###Residual Block\n",
    "def Residual_Block(inputs,\n",
    "                 out_filters,\n",
    "                 kernel_size=(3, 3, 3),\n",
    "                 strides=(1, 1, 1),\n",
    "                 use_bias=False,\n",
    "                 activation=tf.nn.relu6,\n",
    "                 kernel_initializer=tf.keras.initializers.VarianceScaling(distribution='uniform'),\n",
    "                 bias_initializer=tf.zeros_initializer(),\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.001),\n",
    "                 bias_regularizer=None,\n",
    "                 **kwargs):\n",
    "\n",
    "\n",
    "    conv_params={'padding': 'same',\n",
    "                   'use_bias': use_bias,\n",
    "                   'kernel_initializer': kernel_initializer,\n",
    "                   'bias_initializer': bias_initializer,\n",
    "                   'kernel_regularizer': kernel_regularizer,\n",
    "                   'bias_regularizer': bias_regularizer}\n",
    "\n",
    "    in_filters = inputs.get_shape().as_list()[-1]\n",
    "    x=inputs\n",
    "    orig_x=x\n",
    "\n",
    "    ##building\n",
    "    # Adjust the strided conv kernel size to prevent losing information\n",
    "    k = [s * 2 if s > 1 else k for k, s in zip(kernel_size, strides)]\n",
    "\n",
    "    if np.prod(strides) != 1:\n",
    "            orig_x = tf.keras.layers.MaxPool3D(pool_size=strides,strides=strides,padding='valid')(orig_x)\n",
    "\n",
    "    ##sub-unit-0\n",
    "    x=tf.keras.layers.BatchNormalization()(x)\n",
    "    x=activation(x)\n",
    "    x=tf.keras.layers.Conv3D(filters=out_filters,kernel_size=k,strides=strides,**conv_params)(x)\n",
    "\n",
    "    ##sub-unit-1\n",
    "    x=tf.keras.layers.BatchNormalization()(x)\n",
    "    x=activation(x)\n",
    "    x=tf.keras.layers.Conv3D(filters=out_filters,kernel_size=kernel_size,strides=(1,1,1),**conv_params)(x)\n",
    "\n",
    "        # Handle differences in input and output filter sizes\n",
    "    if in_filters < out_filters:\n",
    "        orig_x = tf.pad(tensor=orig_x,paddings=[[0, 0]] * (len(x.get_shape().as_list()) - 1) + [[\n",
    "                    int(np.floor((out_filters - in_filters) / 2.)),\n",
    "                    int(np.ceil((out_filters - in_filters) / 2.))]])\n",
    "\n",
    "    elif in_filters > out_filters:\n",
    "        orig_x = tf.keras.layers.Conv3D(filters=out_filters,kernel_size=kernel_size,strides=(1,1,1),**conv_params)(orig_x)\n",
    "\n",
    "    x += orig_x\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "## Resnet----3D\n",
    "def Resnet3D(inputs,\n",
    "              num_classes,\n",
    "              num_res_units=TRAIN_NUM_RES_UNIT,\n",
    "              filters=TRAIN_NUM_FILTERS,\n",
    "              strides=TRAIN_STRIDES,\n",
    "              use_bias=False,\n",
    "              activation=TRAIN_CLASSIFY_ACTICATION,\n",
    "              kernel_initializer=TRAIN_KERNAL_INITIALIZER,\n",
    "              bias_initializer=tf.zeros_initializer(),\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l=0.001),\n",
    "              bias_regularizer=None,\n",
    "              **kwargs):\n",
    "    conv_params = {'padding': 'same',\n",
    "                   'use_bias': use_bias,\n",
    "                   'kernel_initializer': kernel_initializer,\n",
    "                   'bias_initializer': bias_initializer,\n",
    "                   'kernel_regularizer': kernel_regularizer,\n",
    "                   'bias_regularizer': bias_regularizer}\n",
    "\n",
    "\n",
    "    ##building\n",
    "    k = [s * 2 if s > 1 else 3 for s in strides[0]]\n",
    "\n",
    "\n",
    "    #Input\n",
    "    x = inputs\n",
    "    #1st-convo\n",
    "    x=tf.keras.layers.Conv3D(filters[0], k, strides[0], **conv_params)(x)\n",
    "\n",
    "    for res_scale in range(1, len(filters)):\n",
    "        x = Residual_Block(\n",
    "                inputs=x,\n",
    "                out_filters=filters[res_scale],\n",
    "                strides=strides[res_scale],\n",
    "                activation=activation,\n",
    "                name='unit_{}_0'.format(res_scale))\n",
    "        for i in range(1, num_res_units):\n",
    "            x = Residual_Block(\n",
    "                    inputs=x,\n",
    "                    out_filters=filters[res_scale],\n",
    "                    strides=(1, 1, 1),\n",
    "                    activation=activation,\n",
    "                    name='unit_{}_{}'.format(res_scale, i))\n",
    "\n",
    "\n",
    "    x=tf.keras.layers.BatchNormalization()(x)\n",
    "    x=activation(x)\n",
    "    x=tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "    x =tf.keras.layers.Dropout(0.5)(x)\n",
    "    classifier=tf.keras.layers.Dense(units=num_classes,activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=classifier)\n",
    "    return model\n",
    "\n",
    "\n",
    "model=Resnet3D(layers.Input((128, 128, 64,1)))\n",
    "\n",
    "\n",
    "## This section will be the same for all the classifiers. \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.Precision(),tf.keras.metrics.AUC()])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
